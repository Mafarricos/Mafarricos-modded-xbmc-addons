from stream import plugin
from stream.scrapers import scraper
from stream.ga import tracked
from stream.caching import cached_route
from stream.utils import ensure_fanart
from stream.library import library_context


# Temporary, will be fixed later by them
IMMUNICITY_TPB_URL = "http://thepiratebay.pe"
BASE_URL = "%s/" % (plugin.get_setting("immunicity", bool) and IMMUNICITY_TPB_URL or plugin.get_setting("base_tpb"))
HEADERS = {
    "Referer": BASE_URL,
}


CATEGORIES = [
    ("Audio", 100, [
        ("Music", 101),
        ("Audiobooks", 102),
    ]),

    ("Video", 200, [
        ("Movies - SD", 201),
        ("Movies - DVDR", 202),
        ("Movies - HD", 207),
        ("Movie Clips", 204),
        ("Music Videos", 203),
        ("TV Shows - SD", 205),
        ("TV Shows - HD", 208),
    ]),
]

if plugin.get_setting("porn", bool):
    CATEGORIES += [
        ("Porn", 500, [
            ("Movies - SD", 501),
            ("Movies - HD", 505),
            ("Movie clips", 506),
        ]),
    ]

# Cache TTLs
DEFAULT_TTL = 24 * 3600 # 24 hours


@scraper("Misc")
@plugin.route("/tpb")
@ensure_fanart
@tracked
def piratebay_index():
    yield {"label": "Search", "path": plugin.url_for("piratebay_search")}

    def make_cats(root, prefix=""):
        for cat in root:
            yield {
                "label": "%s%s" % (prefix, cat[0]),
                "path": plugin.url_for("piratebay_page", root="/browse/%d" % cat[1], page=0),
            }
            if len(cat) > 2:
                for entry in make_cats(cat[2], prefix="%s    " % prefix):
                    yield entry

    for cat in make_cats(CATEGORIES):
        yield cat



def piratebay_record(node):
    import re
    from stream.utils import url_get
    from urlparse import urljoin
    from stream import tmdb

    node.seeds, node.peers = map(lambda x: x.text, node.parent.parent.findAll("td")[2:])
    node.magnet_node = node.parent.findAll("a")[1]
    node.desc_node = node.parent.findAll("font", "detDesc")[0]
    node.size = re.search("Size (.*?),", node.desc_node.text).group(1)
    node.txt = "%s (%s S:%s P:%s)" % (node.a.text, node.size.replace("&nbsp;", " "), node.seeds, node.peers)

    node.item = {}
    try:
        node.search_result = url_get(urljoin(BASE_URL, node.parent.findAll("a")[0]["href"]), headers=HEADERS)
    except:
        pass
    else:
        if node.search_result:
            try:
                node.imdb_url = re.search("http://www.imdb.com/title/tt[0-9]*", node.search_result).group(0)
            except:
                pass
            else:
                if node.imdb_url:
                    node.imdb_id = re.search(r"(tt\d+)", node.imdb_url).group(0)

                    if node.imdb_id:
                        node.release_tags = tmdb.get_list_item(tmdb.get(node.imdb_id))

                        if node.release_tags:
                            node.item.update(node.release_tags)

    node.item.update({
        "label": node.txt,
        "path": plugin.url_for("play", uri=node.magnet_node["href"]),
        "is_playable": True,
    })
    return node.item


@plugin.route("/tpb/<root>/<page>")
@library_context
@ensure_fanart
@tracked
def piratebay_page(root, page):
    import xbmc
    import xbmcgui
    from concurrent import futures
    from contextlib import nested, closing
    from bs4 import BeautifulSoup
    from urlparse import urljoin
    from stream.utils import url_get
    from itertools import izip, chain
    from stream.utils import url_get_json, terminating, SafeDialogProgress

    with closing(SafeDialogProgress(delay_close=0)) as dialog:
    	dialog.create(plugin.name)
    	dialog.update(percent=0, line1="Fetching index page...", line2="", line3="")

    page = int(page)

    try:
        html_data = url_get(urljoin(BASE_URL, "%s/%d/7/100,200,500" % (root, page)), headers=HEADERS)
    except:
        dialog = xbmcgui.Dialog()
        dialog.ok("Piratebay","Timeout")
        return

    soup = BeautifulSoup(html_data, "html5lib")
    nodes = soup.findAll("div", "detName")

    state = {"done": 0}
    def on_movie(future):
        data = future.result()
        state["done"] += 1
        dialog.update(
            percent=int(state["done"] * 100.0 / len(nodes)),
            line2=data.get("label") or "",
        )

    dialog.update(percent=0, line1="Fetching movie information...", line2="", line3="")
    with futures.ThreadPoolExecutor(max_workers=10) as pool_tmdb:
        tmdb_list = [pool_tmdb.submit(piratebay_record, node) for node in nodes]
        [future.add_done_callback(on_movie) for future in tmdb_list]
        while not all(job.done() for job in tmdb_list):
                if dialog.iscanceled():
                    return
                xbmc.sleep(200)

    tmdb_list = map(lambda job: job.result(), tmdb_list)
    for node, item in izip(nodes, tmdb_list):
        yield node.item

    yield {
        "label": ">> Next page",
        "path": plugin.url_for("piratebay_page", root=root, page=page + 1),
        "is_playable": False,
    }


@plugin.route("/tpb/search")
@tracked
def piratebay_search():
    import urllib

    query = plugin.request.args.get("query")
    if query:
        query = query[0]
    else:
        query = plugin.keyboard("", "The Pirate Bay - Search")
    if query:
        plugin.redirect(plugin.url_for("piratebay_page", root="/search/%s" % urllib.quote(query, safe=""), page=0))
